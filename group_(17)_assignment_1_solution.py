# -*- coding: utf-8 -*-
"""Group (17) assignment 1 solution.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UsJuKB-RTdrHDO-P_xEqeDfP_fvCwqHs

# ==============================================
# ASSIGNMENT 1: DECONSTRUCTING THE TRANSFORMER
# ==============================================

# Instructions:
## 1. Use Google Colab for all experiments (free GPU tier is sufficient).
## 2. This notebook provides a complementory solution for all parts of the assignment.

# ==============================================
# SUBMISSION INSTRUCTION
# ==============================================

## 1. Please write the name of the file as `Group_(number)_assignemnt_1_solution.ipynb`

##2. Only one member from one group needs to submit the solution, to avoid any duplicasy.

# PART:1

## Tiny Transformer Implementation

You have to complete the code where it's not completed!


'''
Complete the Code
'''

## Imports and Dataset

**We are using language translation dataset for this task (WMT14 DE-EN dataset)**

- Use first 30k samples for training `[You may Increase the training Samples for better results]`

- Use first 5k samples for validation `[You may Increase the validation Samples for better generalization results]`

- Use first 1k samples for testing.

- We have provided the Helping Functions throughout the notebook, you have to complete the Code and Run as per Questions asked in Assignemnt.

- We have Alreday created the `Dataloaders` to get you started quickely and to make sure resulst can be reproduced. Please do not change the Setup and Import Section.
"""

# ============================================
# SETUP AND IMPORTS
# ============================================

# Install required packages
!pip install -q torch matplotlib seaborn numpy
!pip install -q transformers datasets tokenizers
!pip install -q sacrebleu
!pip install bertviz

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.nn.utils.rnn import pad_sequence
from torch.utils.data import DataLoader, Dataset

import math
import time
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Tuple, Optional, List, Dict

from datasets import load_dataset
from tokenizers import Tokenizer
from tokenizers.models import WordLevel
from tokenizers.trainers import WordLevelTrainer
from tokenizers.pre_tokenizers import Whitespace

from collections import Counter
import warnings
warnings.filterwarnings('ignore')

# Set random seed for reproducibility
SEED = 42
torch.manual_seed(SEED)
np.random.seed(SEED)

# Device configuration
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

# ============================================
# DATA LOADING AND PREPROCESSING
# ============================================

# Special tokens
PAD_TOKEN = '<pad>'
SOS_TOKEN = '<sos>'
EOS_TOKEN = '<eos>'
UNK_TOKEN = '<unk>'

# ==============================================================================
#! Change the dataloading samples as asked in the assignemnt.
# ==============================================================================

# ==============================================================================
#! Code Here
# ==============================================================================
# Load WMT14 DE-EN dataset (using a smaller subset)
print("Loading dataset...")
dataset = load_dataset("wmt14", "de-en", split={
    'train': 'train[:30000]',      # Use first ---- samples for training
    'validation': 'validation[:5000]',  # Use first ---- for validation
    'test': 'test[:1000]'          # Use first ---- for testing
})

print(f"Train samples: {len(dataset['train'])}")
print(f"Validation samples: {len(dataset['validation'])}")
print(f"Test samples: {len(dataset['test'])}")

# Example data point
print("\nExample data point:")
print(dataset['train'][0])

# Build tokenizers using Hugging Face tokenizers library
def build_tokenizer(texts: List[str], vocab_size: int = 10000) -> Tokenizer:
    """Build a simple word-level tokenizer"""
    tokenizer = Tokenizer(WordLevel(unk_token=UNK_TOKEN))
    tokenizer.pre_tokenizer = Whitespace()

    trainer = WordLevelTrainer(
        vocab_size=vocab_size,
        special_tokens=[PAD_TOKEN, SOS_TOKEN, EOS_TOKEN, UNK_TOKEN],
        min_frequency=2
    )

    tokenizer.train_from_iterator(texts, trainer)
    return tokenizer

# Extract texts for tokenizer training
print("\nBuilding tokenizers...")
de_texts = [item['translation']['de'] for item in dataset['train']]
en_texts = [item['translation']['en'] for item in dataset['train']]

# Build tokenizers
de_tokenizer = build_tokenizer(de_texts, vocab_size=10000)
en_tokenizer = build_tokenizer(en_texts, vocab_size=10000)

print(f"German vocabulary size: {de_tokenizer.get_vocab_size()}")
print(f"English vocabulary size: {en_tokenizer.get_vocab_size()}")

# Get special token IDs
PAD_IDX = en_tokenizer.token_to_id(PAD_TOKEN)
SOS_IDX = en_tokenizer.token_to_id(SOS_TOKEN)
EOS_IDX = en_tokenizer.token_to_id(EOS_TOKEN)
UNK_IDX = en_tokenizer.token_to_id(UNK_TOKEN)

print(f"\nSpecial token IDs:")
print(f"PAD: {PAD_IDX}, SOS: {SOS_IDX}, EOS: {EOS_IDX}, UNK: {UNK_IDX}")

# ============================================
# DATASET CLASS AND DATA PROCESSING
# ============================================

class TranslationDataset(Dataset):
    """Custom dataset for translation"""

    def __init__(self, data, src_tokenizer, tgt_tokenizer, max_length=100):
        self.data = data
        self.src_tokenizer = src_tokenizer
        self.tgt_tokenizer = tgt_tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        # Get source and target sentences
        src_text = self.data[idx]['translation']['de']
        tgt_text = self.data[idx]['translation']['en']

        # Tokenize
        src_tokens = self.src_tokenizer.encode(src_text)
        tgt_tokens = self.tgt_tokenizer.encode(tgt_text)

        # Truncate if necessary
        src_tokens = src_tokens.ids[:self.max_length-2]  # Leave room for SOS/EOS
        tgt_tokens = tgt_tokens.ids[:self.max_length-2]

        # Add SOS and EOS tokens
        src_tokens = [SOS_IDX] + src_tokens + [EOS_IDX]
        tgt_tokens = [SOS_IDX] + tgt_tokens + [EOS_IDX]

        return torch.tensor(src_tokens), torch.tensor(tgt_tokens)

def collate_fn(batch):
    """Custom collate function to pad sequences"""
    src_batch, tgt_batch = [], []

    for src, tgt in batch:
        src_batch.append(src)
        tgt_batch.append(tgt)

    # Pad sequences
    src_batch = pad_sequence(src_batch, batch_first=True, padding_value=PAD_IDX)
    tgt_batch = pad_sequence(tgt_batch, batch_first=True, padding_value=PAD_IDX)

    return src_batch, tgt_batch

# Create datasets
print("\nCreating datasets...")
train_dataset = TranslationDataset(dataset['train'], de_tokenizer, en_tokenizer)
val_dataset = TranslationDataset(dataset['validation'], de_tokenizer, en_tokenizer)
test_dataset = TranslationDataset(dataset['test'], de_tokenizer, en_tokenizer)

# Create DataLoaders
BATCH_SIZE = 64

train_loader = DataLoader(
    train_dataset,
    batch_size=BATCH_SIZE,
    shuffle=True,
    collate_fn=collate_fn,
    num_workers=2
)

val_loader = DataLoader(
    val_dataset,
    batch_size=BATCH_SIZE,
    shuffle=False,
    collate_fn=collate_fn,
    num_workers=2
)

test_loader = DataLoader(
    test_dataset,
    batch_size=BATCH_SIZE,
    shuffle=False,
    collate_fn=collate_fn,
    num_workers=2
)

# Test data loading
print("\nTesting data loading...")
src_batch, tgt_batch = next(iter(train_loader))
print(f"Source batch shape: {src_batch.shape}")
print(f"Target batch shape: {tgt_batch.shape}")

# Show example
print("\nExample tokenized pair:")
src_example = src_batch[0]
tgt_example = tgt_batch[0]

# Decode tokens back to text
src_tokens = [de_tokenizer.id_to_token(idx.item()) for idx in src_example if idx != PAD_IDX]
tgt_tokens = [en_tokenizer.id_to_token(idx.item()) for idx in tgt_example if idx != PAD_IDX]

print(f"Source tokens: {' '.join(src_tokens[:10])}...")
print(f"Target tokens: {' '.join(tgt_tokens[:10])}...")

# Vocabulary mappings for visualization
de_vocab = de_tokenizer.get_vocab()
en_vocab = en_tokenizer.get_vocab()
de_idx2word = {v: k for k, v in de_vocab.items()}
en_idx2word = {v: k for k, v in en_vocab.items()}

"""# PART:1 (A)

- Implement the following.

  - sinusoidal positional encoding

  -  Multi-head attention

  - scaled-dot product attention

  - Feed-forward layer

  - Encoder and Decoder Layer

  - Encoder and Decoder Block

## Solution 1(A)
"""

from tqdm import trange


class PositionalEncoding(nn.Module):
    """
    Add positional encoding to embeddings
    PE(pos, 2i) = sin(pos/10000^(2i/d_model))
    PE(pos, 2i+1) = cos(pos/10000^(2i/d_model))
    """
    def __init__(self, d_model: int, max_len: int = 5000):
        super().__init__()

        # TODO: Implement positional encoding

        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)
        self.register_buffer('pe', pe)


    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Args:
            x: Tensor of shape (batch_size, seq_len, d_model)
        """
        # TODO: Add positional encoding to input
        return x + self.pe[:, :x.size(1)]

class MultiHeadAttention(nn.Module):
    """
    Multi-Head Attention mechanism
    """
    def __init__(self, d_model: int, n_heads: int, dropout: float = 0.1):
        super().__init__()
        assert d_model % n_heads == 0

        self.d_model = d_model
        self.n_heads = n_heads
        self.d_k = d_model // n_heads

        # TODO: Initialize linear layers for Q, K, V projections and output
        self.fc_q = nn.Linear(d_model, d_model)
        self.fc_k = nn.Linear(d_model, d_model)
        self.fc_v = nn.Linear(d_model, d_model)
        self.fc_o = nn.Linear(d_model, d_model)

        self.dropout = nn.Dropout(dropout)

        # Store attention weights for visualization
        self.attention_weights = None

    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor,
                mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        Args:
            query: (batch_size, seq_len_q, d_model)
            key: (batch_size, seq_len_k, d_model)
            value: (batch_size, seq_len_v, d_model)
            mask: (batch_size, seq_len_q, seq_len_k) or None

        Returns:
            output: (batch_size, seq_len_q, d_model)
        """
        batch_size = query.size(0)

        # TODO: Implement multi-head attention
        Q = self.fc_q(query)
        K = self.fc_k(key)
        V = self.fc_v(value)

        Q = Q.view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)
        K = K.view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)
        V = V.view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)

        x, self.attention_weights = self.scaled_dot_product_attention(Q, K, V, mask)

        x = x.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)

        output = self.fc_o(x)

        # Attention


        # Store weights for visualization


        #  Concatenate heads


        # Final linear projection


        return output

    def scaled_dot_product_attention(self, Q, K, V, mask=None):
        """
        Calculate scaled dot-product attention
        """
        # TODO: Implement scaled dot-product attention
        # Attention(Q,K,V) = softmax(QK^T/√d_k)V
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)

        attention_weights = F.softmax(scores, dim=-1)
        output = torch.matmul(self.dropout(attention_weights), V)


        return output, attention_weights

class FeedForward(nn.Module):
    """
    Feed-forward network (FFN)
    FFN(x) = max(0, xW1 + b1)W2 + b2
    """
    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.1):
        super().__init__()

        # TODO: Implement feed-forward network with 2 linear layrers and dropout
        self.fc1 = nn.Linear(d_model, d_ff)
        self.fc2 = nn.Linear(d_ff, d_model)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x: torch.Tensor) -> torch.Tensor:

        # TODO: Implement forward pass
        return self.fc2(self.dropout(F.relu(self.fc1(x))))

class EncoderLayer(nn.Module):
    """
    Single encoder layer consisting of:
    1. Multi-head self-attention
    2. Feed-forward network
    Both with residual connections and layer normalization
    """
    def __init__(self, d_model: int, n_heads: int, d_ff: int, dropout: float = 0.1):
        super().__init__()

        # TODO: Initialize components
        self.self_attn = MultiHeadAttention(d_model, n_heads, dropout)
        self.ff = FeedForward(d_model, d_ff, dropout)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        # TODO: Implement encoder layer forward pass
        # Remember: residual connections and layer normalization

        # Self-attention block


        # Feed-forward block
        _x = self.self_attn(x, x, x, mask)
        x = self.norm1(x + self.dropout(_x))
        _x = self.ff(x)
        x = self.norm2(x + self.dropout(_x))

        return x

class DecoderLayer(nn.Module):
    """
    Single decoder layer consisting of:
    1. Masked multi-head self-attention
    2. Multi-head cross-attention
    3. Feed-forward network
    All with residual connections and layer normalization
    """
    def __init__(self, d_model: int, n_heads: int, d_ff: int, dropout: float = 0.1):
        super().__init__()

        # TODO: Initialize components
        self.self_attn = MultiHeadAttention(d_model, n_heads, dropout)
        self.cross_attn = MultiHeadAttention(d_model, n_heads, dropout)
        self.ff = FeedForward(d_model, d_ff, dropout)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.norm3 = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x: torch.Tensor, encoder_output: torch.Tensor,
                src_mask: Optional[torch.Tensor] = None,
                tgt_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        # TODO: Implement decoder layer forward pass

        # Masked self-attention block


        # Cross-attention block


        # Feed-forward block
        _x = self.self_attn(x, x, x, tgt_mask)
        x = self.norm1(x + self.dropout(_x))
        _x = self.cross_attn(x, encoder_output, encoder_output, src_mask)
        x = self.norm2(x + self.dropout(_x))
        _x = self.ff(x)
        x = self.norm3(x + self.dropout(_x))

        return x

class Encoder(nn.Module):
    """
    Transformer Encoder consisting of multiple encoder layers
    """
    def __init__(self, n_layers: int, d_model: int, n_heads: int,
                 d_ff: int, dropout: float = 0.1):
        super().__init__()

        # TODO: Create stack of encoder layers with normalization
        self.layers = nn.ModuleList([EncoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(n_layers)])
        self.norm = nn.LayerNorm(d_model)

    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        # TODO: Pass input through all encoder layers
        for layer in self.layers:
            x = layer(x, mask)
        return self.norm(x)         #TODO


class Decoder(nn.Module):
    """
    Transformer Decoder consisting of multiple decoder layers
    """
    def __init__(self, n_layers: int, d_model: int, n_heads: int,
                 d_ff: int, dropout: float = 0.1):
        super().__init__()

        # TODO: Create stack of decoder layers with niormlaization
        self.layers = nn.ModuleList([DecoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(n_layers)])
        self.norm = nn.LayerNorm(d_model)

    def forward(self, x: torch.Tensor, encoder_output: torch.Tensor,
                src_mask: Optional[torch.Tensor] = None,
                tgt_mask: Optional[torch.Tensor] = None) -> torch.Tensor:

        # TODO: Pass input through all decoder layers
        for layer in self.layers:
            x = layer(x, encoder_output, src_mask, tgt_mask)
        return self.norm(x) #TODO

"""## PART - 1(B)

Implement a Tiny Transformer with the following specifications:

    - Embedding dimension: 128

    - Transformer Layers 2

    - Configurable number of attention heads (1,2,4..8 etc)

    - Feed-Forward dim: 512

    - Max-Token Seq Length: 128

## Solution 1(B)
"""

class TinyTransformer(nn.Module):
    """
    Complete Transformer model for translation

    Architecture:
    - Source embedding + positional encoding
    - Target embedding + positional encoding
    - Encoder stack
    - Decoder stack
    - Output projection layer
    """
    def __init__(self, src_vocab_size: int, tgt_vocab_size: int,
                 d_model: int = 256, n_heads: int = 4, n_layers: int = 2,
                 d_ff: int = 1024, max_seq_len: int = 100, dropout: float = 0.1):
        super().__init__()

        # Model parameters
        self.d_model = d_model
        self.n_heads = n_heads

        # TODO: Initialize all components
        # Embeddings

        # Positional encoding

        self.src_embedding = nn.Embedding(src_vocab_size, d_model)
        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)
        self.pos_encoder = PositionalEncoding(d_model, max_seq_len)

        self.encoder = Encoder(n_layers, d_model, n_heads, d_ff, dropout)
        self.decoder = Decoder(n_layers, d_model, n_heads, d_ff, dropout)

        self.fc_out = nn.Linear(d_model, tgt_vocab_size)

        self.dropout = nn.Dropout(dropout)


        # Encoder and Decoder stacks


        # Output projection


        # Dropout


        # Initialize parameters
        self._init_parameters()

    def _init_parameters(self):
        """Initialize parameters with Xavier uniform"""
        for p in self.parameters():
            if p.dim() > 1:
                nn.init.xavier_uniform_(p)

    def create_src_mask(self, src: torch.Tensor) -> torch.Tensor:
        """Create source mask to hide padding tokens"""

        # TODO: Create mask where True indicates valid position

        return (src != PAD_IDX).unsqueeze(1).unsqueeze(2)

    def create_tgt_mask(self, tgt: torch.Tensor) -> torch.Tensor:
        """Create target mask to hide padding tokens and future positions"""
        # TODO: Create mask combining padding mask and causal mask


        # Padding mask


        # Causal mask (lower triangular)
        tgt_pad_mask = (tgt != PAD_IDX).unsqueeze(1).unsqueeze(2)
        tgt_len = tgt.shape[1]
        tgt_sub_mask = torch.tril(torch.ones((tgt_len, tgt_len), device=tgt.device)).bool()
        return tgt_pad_mask & tgt_sub_mask

        # Combine masks


    def encode(self, src: torch.Tensor, src_mask: torch.Tensor) -> torch.Tensor:
        """Encode source sequence"""
        # TODO: Implement encoding
        # 1. Embed source tokens
        # 2. Scale embeddings by sqrt(d_model)
        # 3. Add positional encoding
        # 4. Apply dropout
        # 5. Pass through encoder


        src_emb = self.dropout(self.pos_encoder(self.src_embedding(src) * math.sqrt(self.d_model)))
        return self.encoder(src_emb, src_mask)
        #TODO

    def decode(self, tgt: torch.Tensor, encoder_output: torch.Tensor,
               src_mask: torch.Tensor, tgt_mask: torch.Tensor) -> torch.Tensor:
        """Decode target sequence"""
        # TODO: Implement decoding
        # Similar to encoding but with decoder

        tgt_emb = self.dropout(self.pos_encoder(self.tgt_embedding(tgt) * math.sqrt(self.d_model)))
        # It should call the 'decoder'
        return self.decoder(tgt_emb, encoder_output, src_mask, tgt_mask)


    def forward(self, src: torch.Tensor, tgt: torch.Tensor) -> torch.Tensor:
        """
        Forward pass for training

        Args:
            src: Source sequences (batch_size, src_seq_len)
            tgt: Target sequences (batch_size, tgt_seq_len)

        Returns:
            output: Predicted token logits (batch_size, tgt_seq_len, tgt_vocab_size)
        """
        # TODO: Implement complete forward pass
        # 1. Create masks
        # 2. Encode source
        # 3. Decode target
        # 4. Project to vocabulary
        src_mask = self.create_src_mask(src)
        tgt_mask = self.create_tgt_mask(tgt)

        encoder_output = self.encode(src, src_mask)
        decoder_output = self.decode(tgt, encoder_output, src_mask, tgt_mask)

        return self.fc_out(decoder_output)

    def get_attention_weights(self):
        """
        Extract attention weights from all layers for visualization

        Returns dict with:
        - encoder_self_attention: List of attention weights from encoder layers
        - decoder_self_attention: List of attention weights from decoder self-attention
        - decoder_cross_attention: List of attention weights from decoder cross-attention
        """
        attention_weights = {
            'encoder_self_attention': [],
            'decoder_self_attention': [],
            'decoder_cross_attention': []
        }

        # TODO: Extract attention weights from all layers
        # Encoder self-attention


        # Decoder self-attention and cross-attention
        for layer in self.encoder.layers:
            attention_weights['encoder_self_attention'].append(layer.self_attn.attention_weights)

        for layer in self.decoder.layers:
            attention_weights['decoder_self_attention'].append(layer.self_attn.attention_weights)
            attention_weights['decoder_cross_attention'].append(layer.cross_attn.attention_weights)

        return attention_weights

"""## PART 1(C-D)

- Train multi head model (4 heads) and single head (1) model, by keeping the number of parameters same, adjust attention head dimension accordingly.

- Implement visualization for different attention types like Encoder self-attention, Decoder self-attention and Decoder cross-attention. Visualize the attentions for multi-head and single-head both for given test sentences.

## Solution 1(C-D)
"""

from tqdm import tqdm, trange

# ============================================
# TODO: WRITE TRAINING FUNCTIONS
# ============================================

def train_epoch(model: nn.Module, dataloader: DataLoader, optimizer: optim.Optimizer,
                criterion: nn.Module, clip: float = 1.0) -> float:
    """Train for one epoch"""
    model.train()

    epoch_loss = 0

    for batch in tqdm(dataloader, desc="Training", leave=False):
        src, tgt = batch
        src = src.to(device)
        tgt = tgt.to(device)
        optimizer.zero_grad()

        output = model(src, tgt[:,:-1])

        output_dim = output.shape[-1]
        output = output.contiguous().view(-1, output_dim)
        tgt_for_loss = tgt[:,1:].contiguous().view(-1)

        loss = criterion(output, tgt_for_loss)
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)
        optimizer.step()

        epoch_loss += loss.item()
    return epoch_loss / len(dataloader)

def evaluate(model: nn.Module, dataloader: DataLoader, criterion: nn.Module) -> float:
    """Evaluate model"""
    model.eval()
    epoch_loss = 0
    with torch.no_grad():
        for batch in tqdm(dataloader, desc="Evaluating", leave=False):
            src, tgt = batch
            src = src.to(device)
            tgt = tgt.to(device)
            output = model(src, tgt[:,:-1])

            output_dim = output.shape[-1]
            output = output.contiguous().view(-1, output_dim)
            tgt_for_loss = tgt[:,1:].contiguous().view(-1)

            loss = criterion(output, tgt_for_loss)
            epoch_loss += loss.item()
    return epoch_loss / len(dataloader)

# ============================================
# TODO: WRITE TRANSLATING SENTENCE FUNCTION
# ============================================

def translate_sentence(model: nn.Module, src_sentence: str, max_length: int = 50):
    """Translate a single sentence"""
    model.eval()
    src_tokens = [SOS_IDX] + de_tokenizer.encode(src_sentence).ids + [EOS_IDX]
    src_tensor = torch.LongTensor(src_tokens).unsqueeze(0).to(device)

    tgt_indices = [SOS_IDX]

    for i in range(max_length):
        tgt_tensor = torch.LongTensor(tgt_indices).unsqueeze(0).to(device)
        with torch.no_grad():
            output = model(src_tensor, tgt_tensor)

        pred_token = output.argmax(2)[:,-1].item()
        tgt_indices.append(pred_token)

        if pred_token == EOS_IDX:
            break

    translation_tokens = [en_tokenizer.id_to_token(i) for i in tgt_indices]
    translation = " ".join(translation_tokens)
    return translation, tgt_indices

# ============================================
# TODO: WRITE ATTENTION VISUALIZATION FUNCTION
# ============================================
def plot_attention_matrix(attention, x_labels, y_labels, title):
    fig, ax = plt.subplots(figsize=(8, 8))
    cax = ax.matshow(attention.cpu().numpy(), cmap='bone')
    fig.colorbar(cax)
    ax.set_xticks(range(len(x_labels)))
    ax.set_yticks(range(len(y_labels)))
    ax.set_xticklabels(x_labels, rotation=90)
    ax.set_yticklabels(y_labels)
    ax.set_title(title)
    plt.tight_layout()
    plt.show()


# ========================================================================
# TODO: YOU CAN USE THE ATTENTION VISUALIZATION TOOLS : ADD THE PLOTS HERE
# ========================================================================

def visualize_attention(model: nn.Module, src_sentence: str, tgt_sentence: str = None):
    """
    Visualize attention weights for all heads in all layers
    """
    model.eval()
    translation, tgt_indices = translate_sentence(model, src_sentence)

    src_tokens = [SOS_IDX] + de_tokenizer.encode(src_sentence).ids + [EOS_IDX]
    src_tokens_str = [de_tokenizer.id_to_token(i) for i in src_tokens]
    tgt_tokens_str = [en_tokenizer.id_to_token(i) for i in tgt_indices]

    # Get attention weights
    attention_weights = model.get_attention_weights()

    # 1. Encoder self-attention
    print("--- Encoder Self-Attention ---")
    for layer_idx, attn in enumerate(attention_weights['encoder_self_attention']):
        # We'll visualize the attention for the first head of the first item in the batch
        plot_attention_matrix(attn[0, 0, :, :], src_tokens_str, src_tokens_str, f'Encoder Layer {layer_idx+1}, Head 1')

    # 2. Decoder self-attention
    print("--- Decoder Self-Attention ---")
    for layer_idx, attn in enumerate(attention_weights['decoder_self_attention']):
        plot_attention_matrix(attn[0, 0, :, :], tgt_tokens_str, tgt_tokens_str, f'Decoder Layer {layer_idx+1}, Head 1 (Self)')

    # 3. Decoder cross-attention
    print("--- Decoder Cross-Attention ---")
    for layer_idx, attn in enumerate(attention_weights['decoder_cross_attention']):
        plot_attention_matrix(attn[0, 0, :, :], src_tokens_str, tgt_tokens_str, f'Decoder Layer {layer_idx+1}, Head 1 (Cross)')
    # TODO: Implement visualization for different attention types
    # 1. Encoder self-attention
    # 2. Decoder self-attention
    # 3. Decoder cross-attention



# ===================================================
# TODO: WRITE ATTENTION COMPARISON PATTERNS FUNCTION
# ===================================================

def compare_attention_patterns(model_multi: nn.Module, model_single: nn.Module,
                             src_sentence: str):
    """
    Compare attention patterns between multi-head and single-head models
    """
    print("\n=== COMPARING MULTI-HEAD vs SINGLE-HEAD ATTENTION ===")

  # TODO
    translation_multi, tgt_indices_multi = translate_sentence(model_multi, src_sentence)
    translation_single, tgt_indices_single = translate_sentence(model_single, src_sentence)

    src_tokens = [SOS_IDX] + de_tokenizer.encode(src_sentence).ids + [EOS_IDX]
    src_tokens_str = [de_tokenizer.id_to_token(i) for i in src_tokens]
    tgt_tokens_str_multi = [en_tokenizer.id_to_token(i) for i in tgt_indices_multi]
    tgt_tokens_str_single = [en_tokenizer.id_to_token(i) for i in tgt_indices_single]

    attn_multi = model_multi.get_attention_weights()['decoder_cross_attention'][-1][0] # Last layer, first batch item
    attn_single = model_single.get_attention_weights()['decoder_cross_attention'][-1][0]

    # Plot Multi-head (sum of heads)
    plot_attention_matrix(attn_multi.sum(dim=0), src_tokens_str, tgt_tokens_str_multi, "Multi-Head Cross-Attention (Sum of Heads)")

    # Plot Single-head
    plot_attention_matrix(attn_single[0], src_tokens_str, tgt_tokens_str_single, "Single-Head Cross-Attention")




# ============================================
# TODO: WRITE MAIN TRAINING SCRIPT
# ============================================

def count_parameters(model):
    """Count trainable parameters"""
    return sum(p.numel() for p in model.parameters() if p.requires_grad)

# Model configurations
MODEL_CONFIGS = {
    'multi_head': {
        'd_model': 128,
        'n_heads': 4,
        'n_layers': 2,
        'd_ff': 512,
        'dropout': 0.1
    },
    'single_head': {
        'd_model': 128,
        'n_heads': 1,
        'n_layers': 2,
        'd_ff': 512,
        'dropout': 0.1
    }
}

# Initialize models
print("Initializing models...")

model_multi = TinyTransformer(
    src_vocab_size=len(de_vocab),
    tgt_vocab_size=len(en_vocab),
    **MODEL_CONFIGS['multi_head']
).to(device)

model_single = TinyTransformer(
    src_vocab_size=len(de_vocab),
    tgt_vocab_size=len(en_vocab),
    **MODEL_CONFIGS['single_head']
).to(device)

print(f"Multi-head model parameters: {count_parameters(model_multi):,}")
print(f"Single-head model parameters: {count_parameters(model_single):,}")

# Training settings
LEARNING_RATE = 0.0001 # YOU CAN CHOOSE AS PER TRAINING AND VALIDATION PLOTS
N_EPOCHS = 100# TODO

# Loss function - ignore padding token
criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)

# ============================================
# TRAINING LOOP FOR MULTI-HEAD MODEL
# ============================================



# Training history


# ============================================
# TODO: TRAINING LOOP FOR SINGLE-HEAD MODEL
# ============================================



    # Evaluate




    # Save best model


# Load best model
# TODO

# ============================================
# TODO: PLOT TRAINING CURVES
# ============================================


# ============================================
# TODO: EVALUATION AND VISUALIZATION
# ============================================

print("\n" + "="*50)
print("TRAINING MULTI-HEAD MODEL (4 heads)")
print("="*50)

optimizer_multi = optim.Adam(model_multi.parameters(), lr=LEARNING_RATE)
train_losses_multi = []
val_losses_multi = []

best_val_loss = float('inf')

for epoch in trange(N_EPOCHS, desc="Epochs"):
    start_time = time.time()

    # Train
    train_loss = train_epoch(model_multi, train_loader, optimizer_multi, criterion)

    # Evaluate
    val_loss = evaluate(model_multi, val_loader, criterion)

    end_time = time.time()
    epoch_mins, epoch_secs = divmod(end_time - start_time, 60)

    train_losses_multi.append(train_loss)
    val_losses_multi.append(val_loss)

    # Save best model
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        torch.save(model_multi.state_dict(), 'tiny_transformer_multi.pt')

    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins:.0f}m {epoch_secs:.0f}s')
    print(f'\tTrain Loss: {train_loss:.3f} | Val Loss: {val_loss:.3f}')

# Load best model
model_multi.load_state_dict(torch.load('tiny_transformer_multi.pt'))
checkpoint = {
    'model_state_dict': model_multi.state_dict(),
    'val_losses': val_losses_multi, # The validation loss list
    'train_losses': train_losses_multi # You can also save the training loss
}

# 3. Save the dictionary to a single .pt file
torch.save(checkpoint, 'model_multi_checkpoint.pt')

checkpoint = torch.load('model_multi_checkpoint.pt')

# 3. Load the parameters into the model instance
model_multi.load_state_dict(checkpoint['model_state_dict'])

# 4. Retrieve the validation loss list
loaded_val_losses = checkpoint['val_losses']

print("\n" + "="*50)
print("TRAINING SINGLE-HEAD MODEL")
print("="*50)

optimizer_single = optim.Adam(model_single.parameters(), lr=LEARNING_RATE)

# Training history
train_losses_single = []
val_losses_single = []

best_val_loss = float('inf')# TODO

for epoch in trange(N_EPOCHS):
    start_time = time.time()

    # Train
    train_loss = train_epoch(model_single, train_loader, optimizer_single, criterion)
    val_loss = evaluate(model_single, val_loader, criterion)
    end_time = time.time()
    epoch_mins, epoch_secs = divmod(end_time - start_time, 60)

    train_losses_single.append(train_loss)
    val_losses_single.append(val_loss)

    if val_loss < best_val_loss:
        best_val_loss = val_loss
        torch.save(model_single.state_dict(), 'tiny_transformer_single.pt')
    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins:.0f}m {epoch_secs:.0f}s')
    print(f'\tTrain Loss: {train_loss:.3f} | Val Loss: {val_loss:.3f}')

# Load best model
# TODO

model_single.load_state_dict(torch.load('tiny_transformer_single.pt'))

val_losses_multi = checkpoint['val_losses']
train_losses_multi = checkpoint['train_losses']

plt.figure(figsize=(12, 5))

# Loss curves
plt.subplot(1, 2, 1)
plt.plot(train_losses_multi, label='Multi-head Train')
plt.plot(val_losses_multi, label='Multi-head Val')
plt.plot(train_losses_single, label='Single-head Train')
plt.plot(val_losses_single, label='Single-head Val')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training and Validation Loss')
plt.legend()
plt.grid(True)

# Validation loss comparison
plt.subplot(1, 2, 2)
epochs = range(1, N_EPOCHS + 1)
plt.plot(epochs, val_losses_multi, 'o-', label='Multi-head')
plt.plot(epochs, val_losses_single, 's-', label='Single-head')
plt.xlabel('Epoch')
plt.ylabel('Validation Loss')
plt.title('Validation Loss Comparison')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()

"""# Explanation
### Part 1(D) — Multi-head vs Single-head Results

From the loss curves we see that both models are able to train, but their behaviour differs:

- The **single-head model** fits the training data faster and its training loss goes down more sharply.  
- The **multi-head model** learns more slowly, but the gap in validation loss between them is small. Around epoch 30–40, both models reach their best validation point, after which overfitting starts.  
- This suggests that, in our dataset setting, a single head with larger dimension can perform slightly better in terms of validation loss.  

However, multi-head attention is still useful because it can capture diverse attention patterns. When we look at the attention visualizations, the multi-head model shows multiple complementary focus patterns across heads, while the single-head model tends to concentrate on a narrower alignment.

"""

print("\n" + "="*50)
print("EVALUATION ON TEST SENTENCES")
print("="*50)

# Test sentences
test_sentences = [
    "Ein Mann läuft auf der Straße.",  # A man walks on the street
    "Die Katze sitzt auf dem Tisch.",   # The cat sits on the table
    "Ich liebe dich.",                  # I love you
    "Das Wetter ist heute schön.",      # The weather is nice today
    "Können Sie mir helfen?"            # Can you help me?
]

# Translate with both models
print("\nTranslation Examples:")
for i, src in enumerate(test_sentences):
    print(f"\n{i+1}. Source: {src}")

    trans_multi, _ = translate_sentence(model_multi, src)
    print(f"   Multi-head: {trans_multi}")

    trans_single, _ = translate_sentence(model_single, src)
    print(f"   Single-head: {trans_single}")

# ============================================
# ATTENTION VISUALIZATION FOR TEST SENTENCE
# ============================================

print("\n" + "="*50)
print("ATTENTION VISUALIZATION")
print("="*50)

# Choose a test sentence for detailed visualization
test_sentence = "Die Katze sitzt auf dem Tisch."
print(f"\nVisualizing attention for: '{test_sentence}'")

# Visualize multi-head model attention
print("\n### MULTI-HEAD MODEL ###")
visualize_attention(model_multi, test_sentence)

# Visualize single-head model attention
print("\n### SINGLE-HEAD MODEL ###")
visualize_attention(model_single, test_sentence)

# Compare attention patterns
compare_attention_patterns(model_multi, model_single, test_sentence)

"""# PART: 2 - Architecture Ablation Study

## PART 2(A)

**Study the Role of Residual Connections**

## Solution 2(A)
"""

src_vocab_size = de_tokenizer.get_vocab_size()
tgt_vocab_size = en_tokenizer.get_vocab_size()

# =========================================================================================================================
# TODO: ARCHITECTURAL ABLATION STUDIES [YOU MAY CHOOSE DIFFERENT IMPLEMENTATION STRATEGY AS LONG AS YOU IMPLEMENT AS ASKED]
# =========================================================================================================================

# --- Experiment 1: Removing Residual Connections ---

class EncoderLayerNoResidual(EncoderLayer):
    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:

        _x = self.self_attn(x, x, x, mask)
        # 2. Then, pass _x to the normalization layer
        x = self.norm1(self.dropout(_x))

        # 3. Do the same for the feed-forward layer
        _x = self.ff(x)
        x = self.norm2(self.dropout(_x))
        return x

class DecoderLayerNoResidual(DecoderLayer):
    def forward(self, x: torch.Tensor, encoder_output: torch.Tensor, src_mask: Optional[torch.Tensor] = None, tgt_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        _x = self.self_attn(x, x, x, tgt_mask)
        x = self.norm1(self.dropout(_x))

        # Cross-attention without residual connection
        _x = self.cross_attn(x, encoder_output, encoder_output, src_mask)
        x = self.norm2(self.dropout(_x))

        # Feed-forward without residual connection
        _x = self.ff(x)
        x = self.norm3(self.dropout(_x))
        return x

class EncoderNoResidual(Encoder):
    def __init__(self, n_layers: int, d_model: int, n_heads: int, d_ff: int, dropout: float = 0.1):
        super().__init__(n_layers, d_model, n_heads, d_ff, dropout)
        self.layers = nn.ModuleList([EncoderLayerNoResidual(d_model, n_heads, d_ff, dropout) for _ in range(n_layers)])
        self.norm = nn.LayerNorm(d_model)

class DecoderNoResidual(Decoder):
     def __init__(self, n_layers: int, d_model: int, n_heads: int, d_ff: int, dropout: float = 0.1):
        super().__init__(n_layers, d_model, n_heads, d_ff, dropout)
        self.layers = nn.ModuleList([DecoderLayerNoResidual(d_model, n_heads, d_ff, dropout) for _ in range(n_layers)])
        self.norm = nn.LayerNorm(d_model)

class TransformerNoResidual(TinyTransformer):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        # Replace encoder and decoder with the no-residual versions
        self.encoder = EncoderNoResidual(kwargs['n_layers'], kwargs['d_model'], kwargs['n_heads'], kwargs['d_ff'], kwargs['dropout'])
        self.decoder = DecoderNoResidual(kwargs['n_layers'], kwargs['d_model'], kwargs['n_heads'], kwargs['d_ff'], kwargs['dropout'])
        self._init_parameters()

model_no_residual = TransformerNoResidual(src_vocab_size, tgt_vocab_size, **MODEL_CONFIGS['multi_head']).to(device)
optimizer_no_res = optim.Adam(model_no_residual.parameters(), lr=LEARNING_RATE)
val_losses_no_residual = []

print("--- Training Model with No Residual Connections ---")
for epoch in trange(N_EPOCHS, desc="No Residual Epochs"):
    train_loss = train_epoch(model_no_residual, train_loader, optimizer_no_res, criterion)
    val_loss = evaluate(model_no_residual, val_loader, criterion)
    val_losses_no_residual.append(val_loss)
    print(f"Epoch {epoch+1:02} | Train Loss: {train_loss:.3f} | Val Loss: {val_loss:.3f}")

# Plotting the comparison
plt.figure(figsize=(10, 5))
plt.plot(val_losses_multi, label='Baseline (Multi-head) Val Loss', marker='o')
plt.plot(val_losses_no_residual, label='No Residual Val Loss', marker='x')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Validation Loss: Baseline vs. No Residuals')
plt.legend()
plt.grid(True)
plt.show()

# TODO -- RUN TRAINING

"""# Explanation
### Part 2(A) — Impact of Removing Residual Connections

The plot compares the validation loss of the baseline Transformer (with residuals) against the version without residuals.

- **Baseline model:** Validation loss decreases steadily and reaches a low value before slightly rising, showing effective training.  
- **No residual model:** Validation loss stays high and even increases over epochs, showing the model is unable to optimize well.  

**Answer:** The model without residual connections does **not** train effectively. Residuals are essential in Transformers to stabilize optimization and allow gradients to flow through deep layers.

## IMPLEMNT **Learnable skip weights** AND TRAIN AGAIN TO OBSERVE THE CHANGES.
"""

# --- Experiment 2: IMPLEMENT LEARNABLE SKIP WEIGHTS ---


# TODO: WRITE THE CODE SIMILIARLY

class EncoderLayerLearnableSkip(EncoderLayer):
    def __init__(self, d_model: int, n_heads: int, d_ff: int, dropout: float = 0.1):
        super().__init__(d_model, n_heads, d_ff, dropout)
        # Learnable parameters for each skip connection
        self.w1 = nn.Parameter(torch.tensor(1.0))
        self.w2 = nn.Parameter(torch.tensor(1.0))

    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        _x = self.self_attn(x, x, x, mask)
        x = self.norm1(self.w1 * x + self.dropout(_x)) # Apply learnable weight
        _x = self.ff(x)
        x = self.norm2(self.w2 * x + self.dropout(_x)) # Apply learnable weight
        return x

class EncoderLearnableSkip(Encoder):
    def __init__(self, n_layers: int, d_model: int, n_heads: int, d_ff: int, dropout: float = 0.1):
        super(Encoder, self).__init__()
        self.layers = nn.ModuleList([EncoderLayerLearnableSkip(d_model, n_heads, d_ff, dropout) for _ in range(n_layers)])
        self.norm = nn.LayerNorm(d_model)

class TransformerLearnableSkip(TinyTransformer):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        # For simplicity, only modifying the encoder as requested by the pattern
        self.encoder = EncoderLearnableSkip(kwargs['n_layers'], kwargs['d_model'], kwargs['n_heads'], kwargs['d_ff'], kwargs['dropout'])
        self._init_parameters()

# INITIALIZE AND TRAIN THE MODEL
model_learnable_skip = TransformerLearnableSkip(src_vocab_size, tgt_vocab_size, **MODEL_CONFIGS['multi_head']).to(device)
optimizer_ls = optim.Adam(model_learnable_skip.parameters(), lr=LEARNING_RATE)
val_losses_ls = []
# Dictionary to store the history of weights
weight_history = {f'L{i+1}_w{j+1}': [] for i in range(MODEL_CONFIGS['multi_head']['n_layers']) for j in range(2)}

print("\n--- Training Model with Learnable Skip Connections ---")
for epoch in trange(N_EPOCHS, desc="Learnable Skip Epochs"):
    _ = train_epoch(model_learnable_skip, train_loader, optimizer_ls, criterion)
    val_loss = evaluate(model_learnable_skip, val_loader, criterion)
    val_losses_ls.append(val_loss)

    # Record weights at the end of each epoch
    for i, layer in enumerate(model_learnable_skip.encoder.layers):
        weight_history[f'L{i+1}_w1'].append(layer.w1.item())
        weight_history[f'L{i+1}_w2'].append(layer.w2.item())
    print(f"Epoch {epoch+1:02} | Val Loss: {val_loss:.3f} | L1_w1: {weight_history['L1_w1'][-1]:.3f} | L1_w2: {weight_history['L1_w2'][-1]:.3f}")

# Plotting the evolution of the weights
plt.figure(figsize=(10, 5))
for name, values in weight_history.items():
    plt.plot(range(1, N_EPOCHS + 1), values, label=name, marker='.')
plt.xlabel('Epoch')
plt.ylabel('Weight Value')
plt.title('Evolution of Learnable Skip Weights During Training')
plt.axhline(y=1.0, color='r', linestyle='--', label='Initial Value (1.0)')
plt.legend()
plt.grid(True)
plt.show()

"""# Explanation
### Part 2(A) — Learnable Skip Weights

In this experiment, each residual connection was modified to have a learnable scalar weight `w`, initialized at 1.0.  
The plot shows how these weights evolved during training:

- All weights **increased above 1.0** as training progressed, stabilizing between ~1.7 and ~1.9.  
- This means the model actually **strengthened the skip (identity) path** rather than down-weighting it.  
- No layer showed a consistent reduction of the residual weight below 1.0.  

**Answer:** The model did **not** down-weight the residual path. Instead, it relied heavily on residual connections by increasing their weight, confirming their importance in stabilizing and improving learning.

## IMPLEMNT **Long-Range Skip Connections** AND TRAIN AGAIN TO OBSERVE THE CHANGES.
"""

# --- Experiment 3: IMPLEMENT LONG RANGE SKIP CONNECTION ---


# TODO: WRITE THE CODE SIMILIARLY

class EncoderLongRangeSkip(Encoder):
    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        outputs = []
        current_input = x

        for i, layer in enumerate(self.layers):
            layer_output = layer(current_input, mask)
            outputs.append(layer_output)

            # The input to the next layer is the sum of the last two outputs
            if i > 0:
                current_input = outputs[i] + outputs[i-1]
            else:
                current_input = outputs[i]

        # The final output is the output of the last layer
        return self.norm(outputs[-1])

class TransformerLongRangeSkip(TinyTransformer):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        # Only modifying the encoder for this experiment
        self.encoder = EncoderLongRangeSkip(kwargs['n_layers'], kwargs['d_model'], kwargs['n_heads'], kwargs['d_ff'], kwargs['dropout'])
        self._init_parameters()

# INITIALIZE AND TRAIN THE MODEL
model_long_range = TransformerLongRangeSkip(src_vocab_size, tgt_vocab_size, **MODEL_CONFIGS['multi_head']).to(device)
optimizer_lr = optim.Adam(model_long_range.parameters(), lr=LEARNING_RATE)
val_losses_lr = []

print("\n--- Training Model with Long-Range Skip Connections ---")
for epoch in trange(N_EPOCHS, desc="Long-Range Skip Epochs"):
    train_loss = train_epoch(model_long_range, train_loader, optimizer_lr, criterion)
    val_loss = evaluate(model_long_range, val_loader, criterion)
    val_losses_lr.append(val_loss)
    print(f"Epoch {epoch+1:02} | Train Loss: {train_loss:.3f} | Val Loss: {val_loss:.3f}")

# Plotting the comparison
plt.figure(figsize=(10, 5))
plt.plot(val_losses_multi, label='Baseline Val Loss', marker='o')
plt.plot(val_losses_lr, label='Long-Range Skip Val Loss', marker='x')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Validation Loss: Baseline vs. Long-Range Skip')
plt.legend()
plt.grid(True)
plt.show()

"""# Explanation - 2A - 3
The model still learns effectively. And there is no such significant difference in losses at each epoch, as shown in the graph.

## PART 2(B)

**Study the Role of Feed-Forward Layers**

- Answer the questions as asked in the Assignemnt.

## Solution 2(B)
"""

# --- Experiment 1: Removing Feed-Forward Layers ---

class EncoderLayerNoFFN(EncoderLayer):
    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        _x = self.self_attn(x, x, x, mask)
        x = self.norm1(x + self.dropout(_x))
        # The FFN block is skipped entirely
        return x
        # TODO # Skip FFN

class DecoderLayerNoFFN(DecoderLayer):
    def forward(self, x: torch.Tensor, encoder_output: torch.Tensor, src_mask: Optional[torch.Tensor] = None, tgt_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        # Masked self-attention block
        _x = self.self_attn(x, x, x, tgt_mask)
        x = self.norm1(x + self.dropout(_x))
        # Cross-attention block
        _x = self.cross_attn(x, encoder_output, encoder_output, src_mask)
        x = self.norm2(x + self.dropout(_x))
        # The FFN block is skipped entirely
        return x

class EncoderNoFFN(Encoder):
    def __init__(self, n_layers: int, d_model: int, n_heads: int, d_ff: int, dropout: float = 0.1):
        super().__init__(n_layers, d_model, n_heads, d_ff, dropout)
        # Use the layer type that has no FFN
        self.layers = nn.ModuleList([EncoderLayerNoFFN(d_model, n_heads, d_ff, dropout) for _ in range(n_layers)])
        self.norm = nn.LayerNorm(d_model)

class DecoderNoFFN(Decoder):
     def __init__(self, n_layers: int, d_model: int, n_heads: int, d_ff: int, dropout: float = 0.1):
        super().__init__(n_layers, d_model, n_heads, d_ff, dropout)
        self.layers = nn.ModuleList([DecoderLayerNoFFN(d_model, n_heads, d_ff, dropout) for _ in range(n_layers)])
        self.norm = nn.LayerNorm(d_model)

class TransformerNoFFN(TinyTransformer):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.encoder = EncoderNoFFN(kwargs['n_layers'], kwargs['d_model'], kwargs['n_heads'], kwargs['d_ff'], kwargs['dropout'])
        self.decoder = DecoderNoFFN(kwargs['n_layers'], kwargs['d_model'], kwargs['n_heads'], kwargs['d_ff'], kwargs['dropout'])
        self._init_parameters()

# TODO
model_no_ffn = TransformerNoFFN(src_vocab_size, tgt_vocab_size, **MODEL_CONFIGS['multi_head']).to(device)
params_baseline = count_parameters(model_multi)
params_no_ffn = count_parameters(model_no_ffn)
reduction = 100 * (1 - params_no_ffn / params_baseline)
print(f"1. Parameter Reduction after removing FFNs: {reduction:.2f}%")


# 2. Train this model and plot its validation loss
optimizer_no_ffn = optim.Adam(model_no_ffn.parameters(), lr=LEARNING_RATE)
val_losses_no_ffn = []
print("\n--- Training Model with No Feed-Forward Networks ---")
for epoch in trange(N_EPOCHS, desc="No FFN Epochs"):
    _ = train_epoch(model_no_ffn, train_loader, optimizer_no_ffn, criterion)
    val_loss = evaluate(model_no_ffn, val_loader, criterion)
    val_losses_no_ffn.append(val_loss)
    print(f"Epoch {epoch+1:02} | Val Loss: {val_loss:.3f}")

print("\n2. Plotting Validation Loss Curve:")
plt.figure(figsize=(10, 5))
plt.plot(val_losses_multi, label='Baseline Val Loss', marker='o')
plt.plot(val_losses_no_ffn, label='No FFN Val Loss', marker='x')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Validation Loss: Baseline vs. No FFN')
plt.legend()
plt.grid(True)
plt.show()
# Answer the questions by training the model

"""# Explanation 2B
The plot clearly shows that the model without the FFN sub-layer (the orange line with 'x' markers) performs significantly worse than the baseline model (the blue line with 'o' markers). The baseline model reaches a lower minimum validation loss and maintains a better performance throughout the training process. The model without the FFN sub-layer struggles to bring its loss down and has a final validation loss that is substantially higher.

# FFNs are crucial because:

FFNs use a non-linear activation function (like ReLU) between two linear layers. This allows the model to learn complex patterns that a purely linear model couldn't. It's like needing curves to draw a circle, not just straight lines.

While the attention mechanism considers all words, the FFN processes each word's representation individually. This allows the model to deeply transform the context-aware information from the attention layer, enabling it to better capture the nuances of the data.

# PART:3 (Attention Modulation)

 - Implement Token Distance as an Attention Bias (as asked in assignemnt)

## Soultion 3(A)
"""

# ==============================================================================
# TOOD: ATTENTION MODULATION (Token Distance Bias)
# ==============================================================================

class DistanceAwareMultiHeadAttention(MultiHeadAttention):
    def __init__(self, d_model: int, n_heads: int, dropout: float = 0.1, max_seq_len: int = 128):
        super().__init__(d_model, n_heads, dropout)
        # Create a distance bias matrix that is not a model parameter
        # TOOD

        # We can use a learnable parameter to scale the distance penalty
        # TOOD) # Negative to penalize distance

        # Create a bias tensor. We use the absolute distance.
        # Think how could you make penalty less harsh for closer tokens.
        # Learnable scalar to scale the distance penalty. Initialized to be negative.
        self.distance_scale = nn.Parameter(torch.tensor(-0.1))

        # Create a non-learnable distance matrix based on absolute position difference
        pos = torch.arange(max_seq_len, dtype=torch.float, device=device).unsqueeze(0)
        dist_matrix = torch.abs(pos - pos.T)
        self.register_buffer('distance_bias', dist_matrix)

    def scaled_dot_product_attention(self, Q, K, V, mask=None):
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)

        # Add the distance bias before the mask and softmax
        seq_len = Q.size(-2)
        # The bias is broadcast across the batch and head dimensions
        bias = self.distance_scale * self.distance_bias[:seq_len, :seq_len]
        scores = scores + bias

        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)

        attention_weights = F.softmax(scores, dim=-1)
        output = torch.matmul(self.dropout(attention_weights), V)
        return output, attention_weights

class EncoderLayerDistanceAware(EncoderLayer):
    def __init__(self, d_model: int, n_heads: int, d_ff: int, dropout: float = 0.1, max_seq_len: int = 128):
        super().__init__(d_model, n_heads, d_ff, dropout)
        self.self_attn = DistanceAwareMultiHeadAttention(d_model, n_heads, dropout, max_seq_len)
        self.ff = FeedForward(d_model, d_ff, dropout)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(dropout)

class EncoderDistanceAware(Encoder):
    def __init__(self, n_layers: int, d_model: int, n_heads: int, d_ff: int, dropout: float = 0.1, max_seq_len: int = 128):
        super().__init__(n_layers, d_model, n_heads, d_ff, dropout)
        self.layers = nn.ModuleList([EncoderLayerDistanceAware(d_model, n_heads, d_ff, dropout, max_seq_len) for _ in range(n_layers)])
        self.norm = nn.LayerNorm(d_model)

class TransformerDistanceAware(TinyTransformer):
     def __init__(self, *args, **kwargs):
        max_seq_len = kwargs.get('max_seq_len', 128)
        super().__init__(*args, **kwargs)

        # Only modify the encoder for this experiment for simplicity (You may choose)
        self.encoder = EncoderDistanceAware(kwargs['n_layers'], kwargs['d_model'], kwargs['n_heads'], kwargs['d_ff'], kwargs['dropout'], max_seq_len)
        self._init_parameters()

"""---
### Distance-Based Biasing in Attention

The technique used here is **distance-based biasing**, where a penalty term is added to the pre-softmax attention scores.  
This penalty is proportional to the absolute distance between tokens.

---

#### Standard Attention:
$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$

#### Modified Attention with Distance Bias:
$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}} + B\right)V
$$

Where \(B\) is the **distance bias matrix**, defined as:
$$
B_{ij} = \alpha \cdot |i - j|
$$

- \(i, j\) → positional indices of query and key tokens  
- \(|i - j|\) → absolute relative distance between tokens  
- \(\alpha\) → learnable scalar parameter (`self.distance_scale`), initialized negative (e.g., `-0.1`) so that the bias acts as a **penalty**  

---

**Effect**:  
This modification penalizes attention scores for tokens that are farther apart, encouraging the model to focus more on **nearby tokens**.

## Solution 3(B)
"""

# TODO

# Train the Model

# --- Plot Final Comparison between validation loss 'Baseline (Multi-head)' and 'Distance-Aware Attention' ---
model_dist_aware = TransformerDistanceAware(src_vocab_size, tgt_vocab_size, **MODEL_CONFIGS['multi_head']).to(device)
optimizer_da = optim.Adam(model_dist_aware.parameters(), lr=LEARNING_RATE)
val_losses_da = []

print("\n--- Training Model with Distance-Aware Attention ---")
for epoch in trange(N_EPOCHS, desc="Distance-Aware Epochs"):
    _ = train_epoch(model_dist_aware, train_loader, optimizer_da, criterion)
    val_loss = evaluate(model_dist_aware, val_loader, criterion)
    val_losses_da.append(val_loss)
    # Print the learned scale parameter to see how it changes
    print(f"Epoch {epoch+1:02} | Val Loss: {val_loss:.3f} | Dist Scale: {model_dist_aware.encoder.layers[0].self_attn.distance_scale.item():.4f}")


# Plot a final comparison of validation losses
print("\n--- Final Performance Comparison ---")
plt.figure(figsize=(10, 5))
plt.plot(val_losses_multi, label='Baseline (Multi-head) Val Loss', marker='o')
plt.plot(val_losses_da, label='Distance-Aware Val Loss', marker='x')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Validation Loss: Baseline vs. Distance-Aware Attention')
plt.legend()
plt.grid(True)
plt.show()


# Visualize the attention patterns from this new model
print("\n--- Visualizing Distance-Aware Encoder Self-Attention ---")
test_sentence = "Die Katze sitzt auf dem Tisch."
print(f"Visualizing for sentence: '{test_sentence}'")
# Get the attention weights from the trained model
translation, tgt_indices = translate_sentence(model_dist_aware, test_sentence)
src_tokens = [SOS_IDX] + de_tokenizer.encode(test_sentence).ids + [EOS_IDX]
src_tokens_str = [de_tokenizer.id_to_token(i) for i in src_tokens]
# Get weights from the first layer, first batch item, first head
attn = model_dist_aware.get_attention_weights()['encoder_self_attention'][0][0,0]
plot_attention_matrix(attn, src_tokens_str, src_tokens_str, "Distance-Aware Encoder Self-Attention (Layer 1, Head 1)")



print("\nAssignment complete.")

"""# Expl 3B
---
### Performance Evaluation

The validation loss plots show that the **distance-aware attention model** performs better than the baseline.  
The orange line (distance-aware) remains below the blue line (baseline), indicating improved learning and lower final loss.  
This suggests the distance bias helps the model generalize more effectively.

---

### Attention Pattern Analysis

The attention heatmap from the distance-aware encoder shows a **strong diagonal pattern**:  
- Tokens mostly attend to themselves and nearby tokens.  
- Attention to distant tokens is penalized, leading to a rapid drop-off away from the diagonal.  

In contrast, standard attention may focus strongly on distant but related tokens.  
By prioritizing local context, distance-aware attention introduces an **inductive bias** that is especially useful in tasks where local dependencies matter (e.g., translation, language modeling).

"""

